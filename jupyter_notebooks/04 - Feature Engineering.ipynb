{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Feature Engineering Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Engineer features for Classification, Regression and Cluster models\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* generate a list with variables to engineer\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "\n",
        "\n",
        "* Feature Engineering Transformers\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of current directory the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6h-QeuCa2_U"
      },
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNJOqym1a38e"
      },
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-WRjItbiOs6"
      },
      "outputs": [],
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvwabO0JsmYW"
      },
      "source": [
        "# Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr(df,threshold, figsize=(20,12), font_annot = 8):\n",
        "  if len(df.columns) > 1:\n",
        "    mask = np.zeros_like(df, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    mask[abs(df) < threshold] = True\n",
        "\n",
        "    fig, axes = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                linewidth=0.5\n",
        "                     )\n",
        "    axes.set_yticklabels(df.columns, rotation = 0)\n",
        "    plt.ylim(len(df.columns),0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df,threshold, figsize=(20,12), font_annot = 8):\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "      mask = np.zeros_like(df, dtype=np.bool)\n",
        "      mask[abs(df) < threshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=figsize)\n",
        "      ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                       mask=mask,cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                       linewidth=0.05,linecolor='grey')\n",
        "      \n",
        "      plt.ylim(len(df.columns),0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "  df_corr_spearman = df.corr(method=\"spearman\")\n",
        "  df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "  pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "  print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "  print(pps_score_stats.round(3))\n",
        "\n",
        "  return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,CorrThreshold,PPS_Threshold,\n",
        "                      figsize=(20,12), font_annot=8 ):\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "  print(\"It evaluates monotonic relationship \\n\")\n",
        "  heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "  heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\")\n",
        "  heatmap_pps(df=pps_matrix,threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(TrainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
        "                  df_corr_spearman=df_corr_spearman, \n",
        "                  pps_matrix=pps_matrix,\n",
        "                  CorrThreshold=0.6, PPS_Threshold=0.15,\n",
        "                  figsize=(10,10), font_annot=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "\n",
        "## Custom Variable\n",
        "\n",
        "Using the variables we believe add up to the total size of the house, we will add them together to create a new variable `TotalArea` which will then be run through our feature engineering along with the rest of the data and see if it can be used later to obtain a better prediction of `SalePrice`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.creation import MathFeatures\n",
        "\n",
        "transformer = MathFeatures(\n",
        "    variables=[\n",
        "        '1stFlrSF',\n",
        "        '2ndFlrSF',\n",
        "        'BsmtFinSF1',\n",
        "        'BsmtUnfSF',\n",
        "        'EnclosedPorch',\n",
        "        'GarageArea',\n",
        "        'GrLivArea',\n",
        "        'LotArea',\n",
        "        'MasVnrArea',\n",
        "        'OpenPorchSF',\n",
        "        'TotalBsmtSF',\n",
        "        'WoodDeckSF'\n",
        "    ],\n",
        "    func=['sum'],\n",
        "    new_variables_names=[\n",
        "        'TotalArea'\n",
        "    ]\n",
        ")\n",
        "\n",
        "transformer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = transformer.transform(TrainSet) , transformer.transform(TestSet)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqEHqNwyrucq"
      },
      "source": [
        "## Custom Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuEsBAljDsLO"
      },
      "source": [
        "We'll be using another function provided by Code Institute, it is used for quick feature engineering on numerical and categorical variables to decide which transformation can better transform the distribution shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "masShIoqzf2b"
      },
      "outputs": [],
      "source": [
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set(style=\"whitegrid\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df,analysis_type=None):\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  - used for quick feature engineering on numerical and categorical variables\n",
        "  to decide which transformation can better transform the distribution shape \n",
        "  - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
        "\n",
        "  \"\"\"\n",
        "  check_missing_values(df)\n",
        "  allowed_types= ['numerical', 'ordinal_encoder',  'outlier_winsorizer', 'outlier_trimmer']\n",
        "  check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "  list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "  \n",
        "  \n",
        "  # Loop in each variable and engineer the data according to the analysis type\n",
        "  df_feat_eng = pd.DataFrame([])\n",
        "  for column in df.columns:\n",
        "    # create additional columns (column_method) to apply the methods\n",
        "    df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "    for method in list_column_transformers:\n",
        "      df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "      \n",
        "    # Apply transformers in respectives column_transformers\n",
        "    df_feat_eng,list_applied_transformers = apply_transformers(analysis_type, df_feat_eng, column)\n",
        "\n",
        "    # For each variable, assess how the transformations perform\n",
        "    transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "  return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "  ### Check analyis type\n",
        "  if analysis_type == None:\n",
        "    raise SystemExit(f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "  if analysis_type not in allowed_types:\n",
        "      raise SystemExit(f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "def check_missing_values(df):\n",
        "  if df.isna().sum().sum() != 0:\n",
        "    raise SystemExit(\n",
        "        f\"There is missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "  ### Set suffix colummns acording to analysis_type\n",
        "  if analysis_type=='numerical':\n",
        "    list_column_transformers = [\"log_e\",\"log_10\",\"reciprocal\", \"power\",\"box_cox\",\"yeo_johnson\"]\n",
        "  \n",
        "  elif analysis_type=='ordinal_encoder':\n",
        "    list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "  elif analysis_type=='outlier_winsorizer':\n",
        "    list_column_transformers = ['iqr']\n",
        "\n",
        "  elif analysis_type=='outlier_trimmer':\n",
        "    list_column_transformers = ['iqr']\n",
        "\n",
        "  return list_column_transformers\n",
        "\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "\n",
        "\n",
        "  for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "    df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "\n",
        "  if analysis_type=='numerical':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
        "  \n",
        "  elif analysis_type=='outlier_winsorizer':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
        "\n",
        "  elif analysis_type=='outlier_trimmer':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_OutlierTrimmer(df_feat_eng,column)\n",
        "\n",
        "  elif analysis_type=='ordinal_encoder':\n",
        "    df_feat_eng,list_applied_transformers = FeatEngineering_CategoricalEncoder(df_feat_eng,column)\n",
        "\n",
        "  return df_feat_eng,list_applied_transformers\n",
        "\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "  # For each variable, assess how the transformations perform\n",
        "  print(f\"* Variable Analyzed: {column}\")\n",
        "  print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "  for col in [column] + list_applied_transformers:\n",
        "    \n",
        "    if analysis_type!='ordinal_encoder':\n",
        "      DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "    \n",
        "    else:\n",
        "      if col == column: \n",
        "        DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "      else:\n",
        "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "  plt.figure(figsize=(4, 3))\n",
        "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'],order = df_feat_eng[col].value_counts().index)\n",
        "  plt.xticks(rotation=90) \n",
        "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
        "  plt.show()\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "  sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=axes[0]) \n",
        "  stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "  sns.boxplot(x=df[variable],ax=axes[2])\n",
        "  \n",
        "  axes[0].set_title('Histogram')\n",
        "  axes[1].set_title('QQ Plot')\n",
        "  axes[2].set_title('Boxplot')\n",
        "  fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "  try:  \n",
        "    encoder= OrdinalEncoder(encoding_method='arbitrary', variables = [f\"{column}_ordinal_encoder\"])\n",
        "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "  \n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_ordinal_encoder\"],axis=1,inplace=True)\n",
        "    \n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### Winsorizer iqr\n",
        "  try: \n",
        "    disc=Winsorizer(\n",
        "        capping_method='iqr', tail='both', fold=1.5, variables = [f\"{column}_iqr\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_iqr\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierTrimmer(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### Trimmer iqr\n",
        "  try: \n",
        "    disc=OutlierTrimmer(\n",
        "        capping_method='iqr', tail='both', fold=1.5, variables = [f\"{column}_iqr\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_iqr\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### LogTransformer base e\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_log_e\"])\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_log_e\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_log_e\"],axis=1,inplace=True)\n",
        "\n",
        "    ### LogTransformer base 10\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_log_10\"],base='10')\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_log_10\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_log_10\"],axis=1,inplace=True)\n",
        "\n",
        "  ### ReciprocalTransformer\n",
        "  try:\n",
        "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_reciprocal\"])\n",
        "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_reciprocal\"],axis=1,inplace=True)\n",
        "\n",
        "  ### PowerTransformer\n",
        "  try:\n",
        "    pt = vt.PowerTransformer(variables = [f\"{column}_power\"])\n",
        "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_power\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_power\"],axis=1,inplace=True)\n",
        "\n",
        "  ### BoxCoxTransformer\n",
        "  try:\n",
        "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_box_cox\"])\n",
        "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_box_cox\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_box_cox\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### YeoJohnsonTransformer\n",
        "  try:\n",
        "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yeo_johnson\"])\n",
        "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "  except:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram_and_boxplot(df):\n",
        "  for col in df.columns:\n",
        "    fig, axes = plt.subplots(nrows=2 ,ncols=1 ,figsize=(7,7), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
        "    sns.boxplot(data=df, x=col, ax=axes[0])\n",
        "    sns.histplot(data=df, x=col, kde=True, ax=axes[1])\n",
        "    fig.suptitle(f\"{col} Distribution - Boxplot and Histogram\")\n",
        "    plt.show()\n",
        "\n",
        "    IQR = df[col].quantile(q=0.75) - df[col].quantile(q=0.25)\n",
        "    print(\n",
        "        f\"This is the range where a datapoint is not an outlier: from \"\n",
        "        f\"{(df[col].quantile(q=0.25) - 1.5*IQR).round(2)} to \"\n",
        "        f\"{(df[col].quantile(q=0.75) + 1.5*IQR).round(2)}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yh2FMDd4wWZ"
      },
      "source": [
        "## Feature Engineering Spreadsheet Summary\n",
        "\n",
        "![FeatureEngineeringSpreadsheet](../media/FeatureEngineering.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-9iRH_ZEMoJ"
      },
      "source": [
        "## Dealing with Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GT8e9W_onJj"
      },
      "source": [
        "### Categorical Enconding\n",
        "\n",
        "For categorical encoding we're just looking to convert the variable names into numbers. The QQ Plot and Box plots aren't of any major value here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwG_KMpOonJu"
      },
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ii9nXtJonJv"
      },
      "outputs": [],
      "source": [
        "variables_engineering= ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t32melBMonJy"
      },
      "source": [
        "* Step 2: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJJBWQlconJy"
      },
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mk058LZonJz"
      },
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method for each variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKEuWXrtonJz"
      },
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfAiPd8DonJz"
      },
      "source": [
        "All of the variables have been converted to numbers as expected and nothing appears to be out of place, so we are happy to apply the transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables = variables_engineering)\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkyO7KsuyG5A"
      },
      "source": [
        "### Numerical Transformation\n",
        "\n",
        "Next are the numerical transformations. Here we are looking to apply transformers that normalize the data, by making the data less skewed and more symetrical.\n",
        "\n",
        "Many of the variables that contain zeros, ie the house doesn't have a second floor or a wood deck, will have strong negative kurtosis which we won't want to fix but we can hopefully leave that in place whilst normalizing the rest of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l4eVnHNyG5E"
      },
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-dEF_P4yG5E"
      },
      "outputs": [],
      "source": [
        "variables_engineering = ['1stFlrSF', \n",
        "                    '2ndFlrSF', \n",
        "                    'BsmtFinSF1', \n",
        "                    'BsmtUnfSF', \n",
        "                    'EnclosedPorch', \n",
        "                    'GarageArea', \n",
        "                    'GrLivArea', \n",
        "                    'LotArea', \n",
        "                    'LotFrontage', \n",
        "                    'MasVnrArea', \n",
        "                    'OpenPorchSF', \n",
        "                    'TotalBsmtSF', \n",
        "                    'WoodDeckSF', \n",
        "                    'TotalArea']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUA_uA1_yG5I"
      },
      "source": [
        "* Step 2: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrplTb39yG5I"
      },
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQqqdvd3yG5J"
      },
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZXMUZgEyG5J"
      },
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5TyFpY1yG5J"
      },
      "source": [
        "`1stFlrSF`, `GrLivArea` and `TotalArea` appear to take well to the `log_e` and `yeo_johnson` methods so for now we will use log e for these variables and trim them of the outliers. \n",
        "\n",
        "We don't want to use the winsorizer as from out previous studies like in notebook 2, we saw that some of the outliers have other strong outlier variables. For example, some of the largest `GrLivArea` had very low `SalePrice` in comparison to the rest of the data. It would be safer to cut these records out of the data than putting a cap on them, as they are still likely to effect the data negatively.\n",
        "\n",
        "For `2ndFlrSF`, `BsmtFinSF1`, `BsmtUnfSF`, `GarageArea`, `MasVnrArea`, `OpenPorchSF`, `TotalBsmtSF` and `WoodDeckSF` we will use `power` transformer as it keeps the strong kurtosis that these variables carry with all their zeros whilst normalizing the rest of the data.\n",
        "\n",
        "`EnclosedPorch` doesn't have enough data that aren't zeros to justify using the power transformer on.\n",
        "\n",
        "And `LotArea` and `LotFrontage` whilst they do contain a small number of zeros are better suited to `log_e` and `yeo_johnson` so again we'll for now use log e.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = vt.LogTransformer(\n",
        "    variables = ['1stFlrSF', \n",
        "        'GrLivArea', \n",
        "        'TotalArea', \n",
        "        'LotArea', \n",
        "        'LotFrontage', ])\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = vt.PowerTransformer(\n",
        "    variables = ['2ndFlrSF', \n",
        "        'BsmtFinSF1', \n",
        "        'BsmtUnfSF', \n",
        "        'GarageArea', \n",
        "        'MasVnrArea',\n",
        "        'OpenPorchSF',\n",
        "        'TotalBsmtSF',\n",
        "        'WoodDeckSF', ])\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outlier Trimmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.outliers import OutlierTrimmer\n",
        "\n",
        "variables_engineering = ['SalePrice', 'GrLivArea', 'TotalArea']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_transformed = OutlierTrimmer(capping_method='iqr', tail='both', fold=1.5, variables=variables_engineering)\n",
        "df_transformed = df_transformed.fit_transform(df_engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"========= Before Transformation ========= \\n\")\n",
        "plot_histogram_and_boxplot(TrainSet[['SalePrice', 'GrLivArea', 'TotalArea']])\n",
        "print(\"\\n\\n ========= After Transformation =========\")\n",
        "plot_histogram_and_boxplot(df=df_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* The dataset has {len(TrainSet)} rows, considering outliers.\")\n",
        "\n",
        "transformer = OutlierTrimmer(\n",
        "    variables=['SalePrice', 'GrLivArea', 'TotalArea'],\n",
        "    tail = 'both',\n",
        "    capping_method='iqr',\n",
        "    fold=1.5\n",
        ")\n",
        "\n",
        "transformer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = transformer.transform(TrainSet) , transformer.transform(TestSet)\n",
        "\n",
        "\n",
        "print(f\"* Once it is transformed with OutlierTrimmer, dataset has {len(TrainSet)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kMYDKU2yuT"
      },
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7iQbNn2yuX"
      },
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqF5Z5TGrGXB"
      },
      "outputs": [],
      "source": [
        "# for this transformer, we don't need to select variables, since we need all variables for this transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-pse1Ge2yub"
      },
      "source": [
        "* Step 2: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hnHWkNf2yub"
      },
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJsN5KNe2yuc"
      },
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq-jWVT2e2v3"
      },
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6,selection_method=\"variance\", missing_values='ignore')\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVQW3s0QfasQ"
      },
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZiCdjylhGP-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSUo5fxnozU"
      },
      "source": [
        "# Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9EtvPIPnpYb"
      },
      "source": [
        "\n",
        "Feature Engineering Transformers\n",
        "  * OrdinalEncoder(encoding_method='arbitrary', variables = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'])\n",
        "  * vt.LogTransformer(variables = ['1stFlrSF', 'GrLivArea', 'TotalArea', 'LotArea', 'LotFrontage'])\n",
        "  * vt.PowerTransformer(variables = ['2ndFlrSF', 'BsmtFinSF1', 'BsmtUnfSF', 'GarageArea', 'MasVnrArea', 'OpenPorchSF','TotalBsmtSF', 'WoodDeckSF']) \n",
        "  * OutlierTrimmer(capping_method='iqr', tail='both', fold=1.5, ['SalePrice', 'GrLivArea', 'TotalArea'])\n",
        "  * SmartCorrelatedSelection - Features to drop: "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
